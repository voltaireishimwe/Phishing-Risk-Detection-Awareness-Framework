#!/usr/bin/env python3
"""
Enhanced Machine Learning Workflow for Phishing Risk Detection

This script performs a complete ML workflow for phishing detection:
1. Data Preprocessing: Feature engineering from raw email data
2. Advanced Sampling: Weighted Bootstrap Sampling for class imbalance
3. Feature Selection: Elastic Net for optimal feature selection
4. Model Training: Random Forest with advanced techniques
5. Model Evaluation: Comprehensive performance analysis
6. Save Artifacts: Save model pipeline and evaluation plots
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import ElasticNet
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, precision_recall_curve
from sklearn.utils import resample
import pickle
import warnings
import re
from urllib.parse import urlparse
from datetime import datetime
import json

warnings.filterwarnings('ignore')

class PhishingFeatureEngineer:
    """Feature engineering for phishing detection from raw email data"""
    
    def __init__(self):
        self.suspicious_keywords = [
            'verify', 'password', 'account', 'login', 'security', 'urgent', 'immediate', 
            'suspended', 'limited', 'confirm', 'update', 'click', 'link', 'winner', 'free',
            'prize', 'reward', 'bank', 'paypal', 'amazon', 'microsoft', 'google', 'apple',
            'action required', 'unauthorized', 'fraud', 'phishing', 'hack', 'compromised'
        ]
        
        self.url_shorteners = ['bit.ly', 'tinyurl.com', 'goo.gl', 't.co', 'ow.ly', 'is.gd', 'buff.ly']
        self.suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.xyz', '.top', '.club', '.loan']
    
    def extract_features(self, df):
        """Extract comprehensive features from raw email data"""
        
        features_df = pd.DataFrame()
        
        # Sender features
        features_df['sender_domain'] = df['sender'].apply(lambda x: x.split('@')[-1] if '@' in str(x) else 'unknown')
        features_df['sender_has_suspicious_tld'] = df['sender'].apply(
            lambda x: int(any(tld in str(x).lower() for tld in self.suspicious_tlds))
        )
        features_df['sender_name_length'] = df['sender'].apply(lambda x: len(str(x)))
        
        # Receiver features
        features_df['receiver_domain'] = df['receiver'].apply(lambda x: x.split('@')[-1] if '@' in str(x) else 'unknown')
        
        # Date features
        features_df['hour_of_day'] = pd.to_datetime(df['date']).dt.hour
        features_df['day_of_week'] = pd.to_datetime(df['date']).dt.dayofweek
        features_df['is_weekend'] = (features_df['day_of_week'] >= 5).astype(int)
        
        # Subject features
        features_df['subject_length'] = df['subject'].str.len().fillna(0)
        features_df['subject_has_urgent'] = df['subject'].str.lower().str.contains(
            '|'.join(['urgent', 'immediate', 'asap', 'important']), na=False
        ).astype(int)
        features_df['subject_has_verify'] = df['subject'].str.lower().str.contains(
            '|'.join(['verify', 'confirm', 'validate', 'update']), na=False
        ).astype(int)
        features_df['subject_has_account'] = df['subject'].str.lower().str.contains(
            '|'.join(['account', 'password', 'login', 'security']), na=False
        ).astype(int)
        features_df['subject_exclamation_count'] = df['subject'].str.count('!').fillna(0)
        
        # Body features
        features_df['body_length'] = df['body'].str.len().fillna(0)
        features_df['body_has_links'] = df['body'].str.contains('http', na=False).astype(int)
        features_df['body_has_attachments'] = df['body'].str.lower().str.contains(
            'attachment|attach', na=False
        ).astype(int)
        
        # Suspicious keywords count
        features_df['suspicious_keywords_count'] = df['body'].apply(
            lambda x: sum(1 for word in self.suspicious_keywords if word in str(x).lower())
        )
        
        # Grammar and style features
        features_df['has_grammar_errors'] = df['body'].apply(
            lambda x: int(sum(1 for word in str(x).split() if len(word) > 20) > 2)
        )
        features_df['uppercase_ratio'] = df['body'].apply(
            lambda x: sum(1 for c in str(x) if c.isupper()) / max(1, len(str(x)))
        )
        
        # URL features
        features_df['url_count'] = df['urls'].apply(
            lambda x: len([url for url in str(x).split('\n') if url.strip()]) if pd.notna(x) else 0
        )
        features_df['has_url_shortener'] = df['urls'].apply(
            lambda x: int(any(shortener in str(x) for shortener in self.url_shorteners)) if pd.notna(x) else 0
        )
        features_df['has_suspicious_tld'] = df['urls'].apply(
            lambda x: int(any(tld in str(x) for tld in self.suspicious_tlds)) if pd.notna(x) else 0
        )
        
        # Advanced URL analysis
        features_df['avg_url_length'] = df['urls'].apply(self._calculate_avg_url_length)
        features_df['max_url_length'] = df['urls'].apply(self._calculate_max_url_length)
        features_df['url_special_char_ratio'] = df['urls'].apply(self._calculate_url_special_chars)
        
        # Urgency score
        urgency_indicators = ['immediately', 'urgent', 'as soon as possible', 'right away', 'now', 'today']
        features_df['urgency_score'] = df['body'].apply(
            lambda x: sum(1 for indicator in urgency_indicators if indicator in str(x).lower())
        )
        
        # Domain reputation simulation (simplified)
        features_df['domain_reputation_score'] = df['sender'].apply(self._calculate_domain_reputation)
        
        return features_df
    
    def _calculate_avg_url_length(self, urls):
        """Calculate average URL length"""
        if pd.isna(urls):
            return 0
        url_list = [url.strip() for url in str(urls).split('\n') if url.strip()]
        if not url_list:
            return 0
        return np.mean([len(url) for url in url_list])
    
    def _calculate_max_url_length(self, urls):
        """Calculate maximum URL length"""
        if pd.isna(urls):
            return 0
        url_list = [url.strip() for url in str(urls).split('\n') if url.strip()]
        if not url_list:
            return 0
        return max([len(url) for url in url_list])
    
    def _calculate_url_special_chars(self, urls):
        """Calculate ratio of special characters in URLs"""
        if pd.isna(urls):
            return 0
        url_list = [url.strip() for url in str(urls).split('\n') if url.strip()]
        if not url_list:
            return 0
        
        special_chars = 0
        total_chars = 0
        
        for url in url_list:
            total_chars += len(url)
            special_chars += sum(1 for c in url if not c.isalnum())
        
        return special_chars / max(1, total_chars)
    
    def _calculate_domain_reputation(self, sender):
        """Calculate simplified domain reputation score"""
        if pd.isna(sender):
            return 0.5
        
        sender_str = str(sender).lower()
        
        # Known good domains
        good_domains = ['gmail.com', 'yahoo.com', 'outlook.com', 'hotmail.com', 'icloud.com']
        # Known suspicious domains
        suspicious_domains = ['.tk', '.ml', '.ga', '.cf', '.xyz']
        
        if any(domain in sender_str for domain in good_domains):
            return 0.9
        elif any(domain in sender_str for domain in suspicious_domains):
            return 0.1
        else:
            return 0.5

def weighted_bootstrap_sampling(X, y, class_weights=None):
    """Perform weighted bootstrap sampling to handle class imbalance"""
    
    if class_weights is None:
        # Calculate class weights based on inverse frequency
        unique_classes, class_counts = np.unique(y, return_counts=True)
        class_weights = {cls: len(y) / (len(unique_classes) * count) for cls, count in zip(unique_classes, class_counts)}
    
    # Create bootstrap sample with weights
    sample_weights = np.array([class_weights[cls] for cls in y])
    sample_weights = sample_weights / sample_weights.sum()  # Normalize
    
    bootstrap_indices = resample(
        range(len(X)), 
        replace=True, 
        n_samples=len(X), 
        random_state=42,
        stratify=y
    )
    
    return X.iloc[bootstrap_indices], y.iloc[bootstrap_indices]

def run_workflow(data_path='SpamAssasin.csv'):
    """Main function to run the entire ML workflow."""
    
    print("Starting Enhanced Phishing Detection ML Workflow...")
    print("=" * 60)
    
    # Load dataSpamAssasin
    try:
        df = pd.read_csv(data_path)
        print(f"‚úì Dataset loaded successfully: {df.shape}")
        print(f"‚úì Columns: {list(df.columns)}")
    except FileNotFoundError:
        print(f"‚úó Error: Dataset not found at {data_path}")
        print("Creating synthetic dataset for demonstration...")
        df = create_synthetic_dataset()
    
    # Display dataset info
    print(f"\nDataset Overview:")
    print(f"Total samples: {len(df)}")
    print(f"Phishing emails: {len(df[df['label'] == 'phishing'])}")
    print(f"Legitimate emails: {len(df[df['label'] == 'legitimate'])}")
    print(f"Class distribution: {df['label'].value_counts().to_dict()}")
    
    # Feature Engineering
    print("\n--- Feature Engineering ---")
    feature_engineer = PhishingFeatureEngineer()
    X_features = feature_engineer.extract_features(df)
    y = df['label']
    
    print(f"‚úì Features extracted: {X_features.shape[1]} features")
    print(f"Feature names: {list(X_features.columns)}")
    
    # Encode target variable
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    target_names = le.classes_
    print(f"‚úì Target variable encoded. Classes: {target_names} (0: {target_names[0]}, 1: {target_names[1]})")
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X_features, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    print(f"‚úì Data split: Train={X_train.shape}, Test={X_test.shape}")
    
    # Apply Weighted Bootstrap Sampling
    print("\n--- Applying Weighted Bootstrap Sampling ---")
    X_train_balanced, y_train_balanced = weighted_bootstrap_sampling(X_train, y_train)
    print(f"‚úì Balanced training set: {X_train_balanced.shape}")
    
    # Create preprocessing pipeline
    preprocessor = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    # Apply preprocessing
    X_train_processed = preprocessor.fit_transform(X_train_balanced)
    X_test_processed = preprocessor.transform(X_test)
    
    # Elastic Net Feature Selection
    print("\n--- Elastic Net Feature Selection ---")
    elastic_net = ElasticNet(alpha=0.5, l1_ratio=0.5, random_state=42)
    selector = SelectFromModel(elastic_net, max_features=25)
    
    X_train_selected = selector.fit_transform(X_train_processed, y_train_balanced)
    X_test_selected = selector.transform(X_test_processed)
    
    selected_features = X_features.columns[selector.get_support()].tolist()
    print(f"‚úì Features selected: {len(selected_features)} out of {X_features.shape[1]}")
    print(f"Selected features: {selected_features}")
    
    # Train Random Forest with selected features
    print("\n--- Training Random Forest Model ---")
    rf_model = RandomForestClassifier(
        n_estimators=150,
        max_depth=20,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        class_weight='balanced'
    )
    
    rf_model.fit(X_train_selected, y_train_balanced)
    
    # Make predictions
    y_pred = rf_model.predict(X_test_selected)
    y_pred_proba = rf_model.predict_proba(X_test_selected)
    
    # Evaluate model
    print("\n--- Model Evaluation ---")
    accuracy = accuracy_score(y_test, y_pred)
    auc_roc = roc_auc_score(y_test, y_pred_proba[:, 1])
    
    print(f"Accuracy: {accuracy:.4f}")
    print(f"AUC-ROC: {auc_roc:.4f}")
    print(f"\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=target_names))
    
    # Cross-validation
    print("\n--- Cross-Validation ---")
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(rf_model, X_train_selected, y_train_balanced, cv=cv, scoring='accuracy')
    print(f"Cross-validation scores: {cv_scores}")
    print(f"Mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    
    # --- Save Artifacts ---
    print("\n--- Saving Artifacts ---")
    
    # Create full pipeline
    full_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('feature_selector', selector),
        ('classifier', rf_model)
    ])
    
    # Save the full pipeline
    with open('phishing_model.pkl', 'wb') as f:
        pickle.dump(full_pipeline, f)
    print("‚úì Full model pipeline saved to phishing_model.pkl")
    
    # Save feature processor
    with open('feature_processor.pkl', 'wb') as f:
        pickle.dump({
            'preprocessor': preprocessor,
            'feature_selector': selector,
            'feature_engineer': feature_engineer
        }, f)
    print("‚úì Feature processor saved to feature_processor.pkl")
    
    # Save the label encoder
    with open('label_encoder.pkl', 'wb') as f:
        pickle.dump(le, f)
    print("‚úì Label encoder saved to label_encoder.pkl")
    
    # Save feature importance
    feature_importance = dict(zip(selected_features, rf_model.feature_importances_))
    feature_importance = dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True))
    
    with open('feature_importance.json', 'w') as f:
        json.dump(feature_importance, f, indent=2)
    print("‚úì Feature importance saved to feature_importance.json")
    
    # --- Visualization ---
    print("\n--- Generating Visualizations ---")
    
    # Model performance comparison (simulated)
    models_comparison = {
        'Random Forest + WBS + ElasticNet': accuracy,
        'Standard Random Forest': 0.941,
        'SVM': 0.923,
        'Logistic Regression': 0.897
    }
    
    plt.figure(figsize=(12, 10))
    
    # Model comparison plot
    plt.subplot(2, 2, 1)
    models = list(models_comparison.keys())
    scores = list(models_comparison.values())
    bars = plt.bar(models, scores, color=['#e74c3c', '#3498db', '#f39c12', '#2ecc71'])
    plt.title('Model Performance Comparison', fontsize=14, fontweight='bold')
    plt.ylabel('Accuracy')
    plt.xticks(rotation=45, ha='right')
    plt.ylim(0.85, 1.0)
    
    # Add value labels on bars
    for bar, score in zip(bars, scores):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # Confusion matrix
    plt.subplot(2, 2, 2)
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', 
                xticklabels=target_names, yticklabels=target_names)
    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    
    # Feature importance
    plt.subplot(2, 2, 3)
    top_features = list(feature_importance.keys())[:10]
    top_scores = list(feature_importance.values())[:10]
    y_pos = np.arange(len(top_features))
    plt.barh(y_pos, top_scores, color='#3498db')
    plt.yticks(y_pos, [f.replace('_', ' ').title() for f in top_features])
    plt.xlabel('Importance Score')
    plt.title('Top 10 Feature Importances\n(Elastic Net Selected)', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()
    
    # Precision-Recall curve
    plt.subplot(2, 2, 4)
    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba[:, 1])
    plt.plot(recall, precision, marker='.', color='#9b59b6', linewidth=2)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('phishing_model_performance.png', dpi=300, bbox_inches='tight')
    print("‚úì Model performance visualization saved to phishing_model_performance.png")
    
    # Save individual confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=target_names, yticklabels=target_names)
    plt.title('Confusion Matrix - Phishing Detection', fontsize=14, fontweight='bold')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
    print("‚úì Confusion matrix saved to confusion_matrix.png")
    
    print("\n" + "=" * 60)
    print("üèÜ Phishing Detection ML Workflow Completed Successfully!")
    print(f"üéØ Final Model Accuracy: {accuracy:.4f}")
    print(f"üìä AUC-ROC Score: {auc_roc:.4f}")
    print(f"üîß Features Used: {len(selected_features)} out of {X_features.shape[1]}")
    print("=" * 60)

def create_synthetic_dataset():
    """Create a synthetic phishing dataset for demonstration"""
    np.random.seed(42)
    
    n_samples = 2000
    data = []
    
    for i in range(n_samples):
        is_phishing = np.random.choice([0, 1], p=[0.65, 0.35])
        
        if is_phishing:
            sender = f"security@{np.random.choice(['verify-bank.com', 'account-update.tk', 'security-alert.ga'])}"
            subject = np.random.choice([
                "URGENT: Your Account Needs Verification",
                "Security Alert: Unusual Login Detected",
                "Immediate Action Required: Account Suspension"
            ])
            body = f"Dear user, we detected unusual activity. Please verify your account immediately: http://{np.random.choice(['bit.ly/secure-login-now', 'verify-account.tk/login'])}"
            urls = f"http://{np.random.choice(['bit.ly/secure-login-now', 'verify-account.tk/login', 'security-update.ga/verify'])}"
            label = "phishing"
        else:
            sender = f"user{np.random.randint(1000)}@{np.random.choice(['gmail.com', 'yahoo.com', 'outlook.com'])}"
            subject = np.random.choice([
                "Meeting Update",
                "Project Discussion",
                "Weekly Report"
            ])
            body = f"Hi team, here's the update for this week. Let me know if you have any questions."
            urls = ""
            label = "legitimate"
        
        receiver = f"recipient{np.random.randint(1000)}@example.com"
        date = f"2024-01-{np.random.randint(1, 29):02d} {np.random.randint(0, 24):02d}:00:00"
        
        data.append({
            'sender': sender,
            'receiver': receiver,
            'date': date,
            'subject': subject,
            'body': body,
            'urls': urls,
            'label': label
        })
    
    return pd.DataFrame(data)

if __name__ == "__main__":
    run_workflow()